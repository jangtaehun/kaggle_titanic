{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502fd2fe-ea0a-42b0-8295-14bdbc67baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    \n",
    "    precision = precision_score(y_test, pred, pos_label=1)\n",
    "    recall = recall_score(y_test, pred, pos_label=1)\n",
    "    f1 = f1_score(y_test, pred, pos_label=1)\n",
    "    \n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    \n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4871886e-a41a-4e03-86f9-ae5074ebdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3fc46cd-cc3a-4138-86b2-922daa31367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/kq4ytf3n0rx3163p8dqq9x080000gn/T/ipykernel_59354/201761695.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df['Age'].fillna(train_df.groupby('Title')['Age'].transform('median'), inplace=True)\n",
      "/var/folders/7p/kq4ytf3n0rx3163p8dqq9x080000gn/T/ipykernel_59354/201761695.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  predict_df['Age'].fillna(predict_df.groupby('Title')['Age'].transform('median'), inplace=True)\n",
      "/var/folders/7p/kq4ytf3n0rx3163p8dqq9x080000gn/T/ipykernel_59354/201761695.py:70: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  predict_df['Fare'].fillna(predict_df['Fare'].dropna().median(), inplace=True) # NaN 값을 median 값으로 대체한 후 train_data와 마찬가지로 분류\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_df = pd.read_csv('./titanic/titanic_train.csv')\n",
    "predict_df = pd.read_csv('./titanic/test.csv')\n",
    "gender_submission_df = pd.read_csv('./titanic/gender_submission.csv')\n",
    "\n",
    "\n",
    "# First Name\n",
    "train_df['Title'] = train_df.Name.str.extract(r'([A-Za-z]+)\\.', expand=False)\n",
    "predict_df['Title'] = predict_df.Name.str.extract(r'([A-Za-z]+)\\.', expand=False)\n",
    "train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', \\\n",
    "                                                         'Dona'], 'Rare')\n",
    "train_df['Title'] = train_df['Title'].replace(['Mlle'], 'Miss')\n",
    "train_df['Title'] = train_df['Title'].replace(['Ms'], 'Miss')\n",
    "train_df['Title'] = train_df['Title'].replace(['Mme'], 'Mrs')\n",
    "\n",
    "predict_df['Title'] = predict_df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', \\\n",
    "                                                         'Dona'], 'Rare')\n",
    "predict_df['Title'] = predict_df['Title'].replace(['Mlle'], 'Miss')\n",
    "predict_df['Title'] = predict_df['Title'].replace(['Ms'], 'Miss')\n",
    "predict_df['Title'] = predict_df['Title'].replace(['Mme'], 'Mrs')\n",
    "\n",
    "\n",
    "# Age\n",
    "average_pclass = train_df.groupby('Pclass')['Age'].mean()\n",
    "# train_df['Age'] = train_df.apply(lambda row: average_pclass[row['Pclass']] if pd.isna(row['Age']) else row['Age'], axis=1)\n",
    "# predict_df['Age'] = predict_df.apply(lambda row: average_pclass[row['Pclass']] if pd.isna(row['Age']) else row['Age'], axis=1)\n",
    "train_df['Age'].fillna(train_df.groupby('Title')['Age'].transform('median'), inplace=True)\n",
    "predict_df['Age'].fillna(predict_df.groupby('Title')['Age'].transform('median'), inplace=True)\n",
    "\n",
    "\n",
    "def get_category(age):\n",
    "    cat = ''\n",
    "    if age <= -1: cat = 'Unknown'\n",
    "    elif age <= 8: cat = 'Baby'\n",
    "    elif age <= 13: cat = 'Child'\n",
    "    elif age <= 19: cat = 'Teenager'\n",
    "    elif age <= 26: cat = 'Student'\n",
    "    elif age <= 39: cat = 'Young Adult'\n",
    "    elif age <= 64: cat = 'Adult'\n",
    "    else: cat = 'Elderly'        \n",
    "    return cat\n",
    "\n",
    "group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']\n",
    " \n",
    "train_df['Age_range'] = train_df['Age'].apply(lambda x : get_category(x))\n",
    "predict_df['Age_range'] = predict_df['Age'].apply(lambda x : get_category(x))\n",
    "\n",
    "\n",
    "# Fare\n",
    "def get_category(fare):\n",
    "    cat = ''\n",
    "    if fare >= 30: cat = 1\n",
    "    elif fare >= 12: cat = 2\n",
    "    else: cat = 3\n",
    "    return cat\n",
    "\n",
    "group_names = [1, 2, 3]\n",
    " \n",
    "train_df['Fare_range'] = train_df['Fare'].apply(lambda x : get_category(x))\n",
    "predict_df['Fare_range'] = predict_df['Fare'].apply(lambda x : get_category(x))\n",
    "\n",
    "# Fare Bend -> data Binning\n",
    "train_df['Fare_Bend'] = pd.qcut(train_df['Fare'], 5, labels=[0,1,2,3,4])\n",
    "# train_df.groupby('Fare_Band', as_index=False)['Survived'].mean().sort_values(by='Fare_Band', ascending=True)\n",
    "\n",
    "predict_df['Fare'].fillna(predict_df['Fare'].dropna().median(), inplace=True) # NaN 값을 median 값으로 대체한 후 train_data와 마찬가지로 분류\n",
    "predict_df.loc[(predict_df[\"Fare\"] <= 7.854), \"Fare_Band\"] = 0\n",
    "predict_df.loc[(predict_df[\"Fare\"] > 7.854) & (predict_df[\"Fare\"] <= 10.500), \"Fare_Band\"] = 1\n",
    "predict_df.loc[(predict_df[\"Fare\"] > 10.5) & (predict_df[\"Fare\"] <= 21.679), \"Fare_Band\"] = 2\n",
    "predict_df.loc[(predict_df[\"Fare\"] > 21.679) & (predict_df[\"Fare\"] <= 39.688), \"Fare_Band\"] = 3\n",
    "predict_df.loc[(predict_df[\"Fare\"] > 39.688), \"Fare_Band\"] = 4\n",
    "\n",
    "\n",
    "# pclass, sex, age_range merge\n",
    "# plcass_sex = [\n",
    "#     # 'Pclass', 'Sex', 'Age_range'] -> Age_range를 추가했을 때 label encoding에서 오류가 발생해 Age_range는 제거하고 진행\n",
    "#     'Pclass', 'Sex']\n",
    "# print(plcass_sex, '\\n')\n",
    "\n",
    "# train_df['plcass_sex'] = train_df[plcass_sex].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "# predict_df['plcass_sex'] = predict_df[plcass_sex].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# plcass_sex_to_drop = ['Pclass', 'Sex']\n",
    "# train_df = train_df.drop(columns=[col for col in plcass_sex_to_drop if col in train_df.columns])\n",
    "# predict_df = predict_df.drop(columns=[col for col in plcass_sex_to_drop if col in predict_df.columns])\n",
    "\n",
    "\n",
    "# Embarked NaN 값 처리\n",
    "train_df.loc[train_df['Embarked'].isna(), 'Embarked'] = 'C'\n",
    "\n",
    "\n",
    "# Familly\n",
    "train_df['Familly'] = train_df['SibSp'] + train_df['Parch'] + 1 # 1 = 자기 자신\n",
    "predict_df['Familly'] = predict_df['SibSp'] + predict_df['Parch'] + 1\n",
    "# train_df.groupby('Familly', as_index=False)['Survived'].mean().sort_values(by=\"Familly\")\n",
    "\n",
    "# 불필요한 특성, null 값 제거\n",
    "def drop_features(df):\n",
    "    df.drop(['PassengerId','Name', 'Ticket', 'Cabin', 'Fare', 'Fare_range', 'Fare_Bend', 'Familly'], axis=1, inplace=True)\n",
    "\n",
    "    y = df['Survived']\n",
    "    df = df.drop('Survived', axis=1, inplace=False)\n",
    "    return df, y\n",
    "\n",
    "\n",
    "X, y = drop_features(train_df)\n",
    "feature = X.columns\n",
    "predict_df = predict_df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.groupby('Fare_Band', as_index=False)['Survived'].mean().sort_values(by='Fare_Band', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7f20b1-ff3b-4a61-8560-d1e7fb80adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.groupby('Familly', as_index=False)['Survived'].mean().sort_values(by=\"Familly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd64ee3b-8636-4ceb-8c5f-8532971259a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mr', 'Mrs', 'Miss', 'Master', 'Rare'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b8ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Rare</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Teenager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Young Adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass     Sex   Age  SibSp  Parch Embarked Title    Age_range\n",
       "0         3    male  22.0      1      0        S    Mr      Student\n",
       "1         1  female  38.0      1      0        C   Mrs  Young Adult\n",
       "2         3  female  26.0      0      0        S  Miss      Student\n",
       "3         1  female  35.0      1      0        S   Mrs  Young Adult\n",
       "4         3    male  35.0      0      0        S    Mr  Young Adult\n",
       "..      ...     ...   ...    ...    ...      ...   ...          ...\n",
       "886       2    male  27.0      0      0        S  Rare  Young Adult\n",
       "887       1  female  19.0      0      0        S  Miss     Teenager\n",
       "888       3  female  21.0      1      2        S  Miss      Student\n",
       "889       1    male  26.0      0      0        C    Mr      Student\n",
       "890       3    male  32.0      0      0        Q    Mr  Young Adult\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4fc6ed-d52e-4b64-8ffd-34dbfc52e723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked', 'Title',\n",
       "       'Age_range'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1983c457-9150-4e77-ac62-71bdce7a3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩 구현\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Age_range']\n",
    "features = ['Sex', 'Embarked', 'Age_range', 'Title']\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in features:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(X[i])\n",
    "    \n",
    "    X[i] = encoder.transform(X[i])\n",
    "    predict_df[i] = encoder.transform(predict_df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f8f005-2c4a-4723-b94d-e3060756f0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df 데이터의 행 개수: 891\n",
      "train_df: 데이터 세트 Null 값 갯수  0\n",
      "Pclass       0\n",
      "Sex          0\n",
      "Age          0\n",
      "SibSp        0\n",
      "Parch        0\n",
      "Embarked     0\n",
      "Title        0\n",
      "Age_range    0\n",
      "dtype: int64\n",
      "Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked', 'Title',\n",
      "       'Age_range'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df 데이터의 행 개수:\", len(X))\n",
    "print('train_df: 데이터 세트 Null 값 갯수 ',X.isnull().sum().sum())\n",
    "print(X.isnull().sum())\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd99bb91-d3fc-43ea-b02e-e913cc8854ce",
   "metadata": {},
   "source": [
    "# 모든 feature 라벨인코딩 -> 스케일링 필요?\n",
    "\n",
    "> -  순서가 없는 범주형 데이터에 적용하면 문제\n",
    "> -  거리 기반 알고리즘이나 신경망을 사용할 경우 스케일링이 필요\n",
    "> -  트리 기반 모델을 사용할 경우에는 필요하지 불필요\n",
    "\n",
    "1. 거리 기반 알고리즘 (예: K-최근접 이웃(K-NN), K-평균 클러스터링)\n",
    "    - 필요성: 매우 높음. 피처 간의 거리 계산에 라벨 인코딩된 값이 큰 영향을 줄 수 있으므로, 스케일링이 필요합니다.\n",
    "2. 선형 모델 (예: 로지스틱 회귀, 선형 회귀)\n",
    "    - 필요성: 보통 높음. 라벨 인코딩된 값이 모델의 가중치 추정에 영향을 미칠 수 있으므로, 스케일링이 유리할 수 있습니다.\n",
    "3. 트리 기반 모델 (예: 결정 트리, 랜덤 포레스트, 그라디언트 부스팅)\n",
    "    - 필요성: 낮음. 트리 기반 모델은 피처의 스케일에 크게 영향을 받지 않으므로, 스케일링이 필요하지 않습니다.\n",
    "4. 신경망 (예: MLP, RNN, CNN)\n",
    "    - 필요성: 높음. 신경망의 학습 과정에서 가중치 업데이트의 안정성을 위해 피처 스케일링이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43581432-b8e6-4921-8273-45cb0b9c86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# numerical_columns = ['Age']\n",
    "numerical_columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'Age_range']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[numerical_columns] = scaler.fit_transform(X[numerical_columns])\n",
    "predict_df[numerical_columns] = scaler.transform(predict_df[numerical_columns])\n",
    "\n",
    "X = pd.DataFrame(X, columns=columns)\n",
    "predict_df = pd.DataFrame(predict_df, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97158c6-28b0-4ced-9da3-c5d2f2a5fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-1.929248</td>\n",
       "      <td>1.401453</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>-1.136545</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>1.401453</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>-0.369365</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>2.670453</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>-1.136545</td>\n",
       "      <td>0.440427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>2.008933</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>-1.136545</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-1.929248</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.669937</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pclass       Sex     SibSp     Parch  Embarked     Title  Age_range\n",
       "0    0.827377  0.737695  0.432793 -0.473674  0.589375  0.132454   0.000986\n",
       "1   -1.566107 -1.355574  0.432793 -0.473674 -1.929248  1.401453   0.879868\n",
       "2    0.827377 -1.355574 -0.474545 -0.473674  0.589375 -1.136545   0.000986\n",
       "3   -1.566107 -1.355574  0.432793 -0.473674  0.589375  1.401453   0.879868\n",
       "4    0.827377  0.737695 -0.474545 -0.473674  0.589375  0.132454   0.879868\n",
       "..        ...       ...       ...       ...       ...       ...        ...\n",
       "886 -0.369365  0.737695 -0.474545 -0.473674  0.589375  2.670453   0.879868\n",
       "887 -1.566107 -1.355574 -0.474545 -0.473674  0.589375 -1.136545   0.440427\n",
       "888  0.827377 -1.355574  0.432793  2.008933  0.589375 -1.136545   0.000986\n",
       "889 -1.566107  0.737695 -0.474545 -0.473674 -1.929248  0.132454   0.000986\n",
       "890  0.827377  0.737695 -0.474545 -0.473674 -0.669937  0.132454   0.879868\n",
       "\n",
       "[891 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff1d2a6-2f2c-4379-ab11-69e214c013ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "886    0\n",
       "887    1\n",
       "888    0\n",
       "889    1\n",
       "890    0\n",
       "Name: Survived, Length: 891, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e820396e-d06d-4cef-82a9-34631086412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.669937</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>1.401453</td>\n",
       "      <td>-1.756776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.369365</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.669937</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>-1.756776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>0.767630</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>1.401453</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-1.355574</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-1.929248</td>\n",
       "      <td>2.670453</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.589375</td>\n",
       "      <td>0.132454</td>\n",
       "      <td>0.879868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>0.767630</td>\n",
       "      <td>-1.929248</td>\n",
       "      <td>-2.405544</td>\n",
       "      <td>-1.317336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pclass       Sex     SibSp     Parch  Embarked     Title  Age_range\n",
       "0    0.827377  0.737695 -0.474545 -0.473674 -0.669937  0.132454   0.879868\n",
       "1    0.827377 -1.355574  0.432793 -0.473674  0.589375  1.401453  -1.756776\n",
       "2   -0.369365  0.737695 -0.474545 -0.473674 -0.669937  0.132454  -1.756776\n",
       "3    0.827377  0.737695 -0.474545 -0.473674  0.589375  0.132454   0.879868\n",
       "4    0.827377 -1.355574  0.432793  0.767630  0.589375  1.401453   0.000986\n",
       "..        ...       ...       ...       ...       ...       ...        ...\n",
       "413  0.827377  0.737695 -0.474545 -0.473674  0.589375  0.132454   0.879868\n",
       "414 -1.566107 -1.355574 -0.474545 -0.473674 -1.929248  2.670453   0.879868\n",
       "415  0.827377  0.737695 -0.474545 -0.473674  0.589375  0.132454   0.879868\n",
       "416  0.827377  0.737695 -0.474545 -0.473674  0.589375  0.132454   0.879868\n",
       "417  0.827377  0.737695  0.432793  0.767630 -1.929248 -2.405544  -1.317336\n",
       "\n",
       "[418 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630534a0-367d-4960-96fe-9a12ac8e7930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['Survived'])['Survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ac0d58-4e07-4224-bc47-dd564c01cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# lightGBM에서 사용\n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=RANDOM_STATE)\n",
    "\n",
    "# from imblearn.combine import SMOTETomek\n",
    "# from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# smoteto = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=RANDOM_STATE)\n",
    "# X_train, y_train = smoteto.fit_resample(X_train, y_train)\n",
    "\n",
    "# print('피처 데이터 shape:{0}'.format(X_train.shape))\n",
    "# print('피처 데이터 shape:{0}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eebb2-96ba-45f7-b267-51a960f1db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf6091-fce0-4ded-b74a-53fcd3a0f91b",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39edc553-0c91-40ad-907a-0495806f109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth, min_samples_split, min_samples_leaf, n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab436c5-b1b2-4bd4-9091-54eb0f201170",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSurvived\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# feature importance를 column 별로 시각화 하기 \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrf_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m pred \u001b[38;5;241m=\u001b[39m rf_clf\u001b[38;5;241m.\u001b[39mpredict(X_train) \n\u001b[1;32m     18\u001b[0m proba \u001b[38;5;241m=\u001b[39m rf_clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_train)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/categorical.py:2341\u001b[0m, in \u001b[0;36mbarplot\u001b[0;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlen\u001b[39m:\n\u001b[1;32m   2339\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2341\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_CategoricalAggPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2351\u001b[0m     ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/categorical.py:67\u001b[0m, in \u001b[0;36m_CategoricalPlotter.__init__\u001b[0;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     58\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/seaborn/_core/data.py:265\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    260\u001b[0m             ids[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(val)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Construct a tidy plot DataFrame. This will convert a number of\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# types automatically, aligning on index in case of pandas objects\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# TODO Note: this fails when variable specs *only* have scalars!\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, names, ids\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# feature importance 추출 \n",
    "feature_names = train_df.columns.drop('Survived')\n",
    "\n",
    "# feature importance를 column 별로 시각화 하기 \n",
    "sns.barplot(x=rf_clf.feature_importances_ , y=numerical_columns)\n",
    "\n",
    "\n",
    "pred = rf_clf.predict(X_train) \n",
    "proba = rf_clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "rf_pred = rf_clf.predict(X_test) \n",
    "rf_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test, rf_pred, rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1275b8e-7ae9-4a48-bb22-c148ac57a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': np.arange(1, 10, 1),\n",
    "    'min_samples_leaf' : np.arange(1, 40, 1),\n",
    "    'min_samples_split' : np.arange(2, 40, 1)\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, oob_score=True, random_state=RANDOM_STATE)\n",
    "grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))\n",
    "\n",
    "best_rf = grid_cv.best_estimator_\n",
    "\n",
    "# feature importance 추출 \n",
    "feature_names = train_df.columns.drop('Survived')\n",
    "\n",
    "# feature importance를 column 별로 시각화 하기 \n",
    "sns.barplot(x=best_rf.feature_importances_ , y=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf55f1c-def1-4aa5-a5d6-489ad4d912e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_clf.predict(X_train) \n",
    "proba = rf_clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = rf_clf.predict(X_test) \n",
    "best_rf_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test, best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1efbf-8a8f-47cb-ba11-477b633dd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_rf = rf_clf.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_rf\n",
    "gender_submission_df.to_csv('titanic_submission_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41830c55-3fa0-46b8-9bac-cac69f304c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4524bc67-0ddc-4b86-ac89-144c9f2189cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True, random_state=RANDOM_STATE)\n",
    "r=[0.0001,0.001,0.1,1,10,50,100]\n",
    "PSVM = [{'C': r, 'kernel': ['linear']},\n",
    "        {'C': r, 'gamma': r, 'kernel': ['rbf']}]\n",
    "\n",
    "GSSVM=GridSearchCV(estimator=svc, param_grid=PSVM, scoring='accuracy', cv=2)\n",
    "scores_svm=cross_val_score(GSSVM, X_train.astype(float), y_train,scoring='accuracy', cv=5)\n",
    "GSSVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d30cb-59d8-4e8c-84ed-645754aee91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = GSSVM.predict(X_train) \n",
    "proba = GSSVM.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = GSSVM.predict(X_test) \n",
    "best_rf_proba = GSSVM.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test, best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b34e2-71e6-4591-af00-2241809f7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_GSSVM = GSSVM.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_GSSVM\n",
    "gender_submission_df.to_csv('titanic_submission_GSSVM.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441e9d3-c97c-4334-a7a1-7961dbedc425",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d382ec-897a-4d78-9a65-555a54d4bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "# xgb = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss', random_state=RANDOM_STATE)\n",
    "xgb = XGBClassifier(random_state=RANDOM_STATE)\n",
    "xgb.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "xgb_preds = xgb.predict(X_test)\n",
    "xgb_pred_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_test , xgb_preds, xgb_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plot_importance(xgb, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99e7fe-b792-400c-8bff-41f7371da774",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb.predict(X_train) \n",
    "proba = xgb.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = xgb.predict(X_test) \n",
    "best_rf_proba = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test , best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba530bd4-8187-43e7-a248-78aee755d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_xgb = xgb.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_xgb\n",
    "gender_submission_df.to_csv('titanic_submission_xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77f609-b867-46f6-9c34-d35f0cf5d0f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_search_space = {'max_depth': hp.quniform('max_depth', 2, 15, 1), \n",
    "                    'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
    "                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)}\n",
    "\n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimators=100,\n",
    "                            max_depth=int(search_space['max_depth']),\n",
    "                            min_child_weight=int(search_space['min_child_weight']),\n",
    "                            colsample_bytree=search_space['colsample_bytree'],\n",
    "                            learning_rate=search_space['learning_rate'],\n",
    "                            early_stopping_rounds=30,\n",
    "                            eval_metric='logloss',\n",
    "                           random_state=RANDOM_STATE)\n",
    "    \n",
    "    roc_auc_list= []\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "        \n",
    "        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)\n",
    "        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n",
    "        roc_auc_list.append(score)\n",
    "    return -1 * np.mean(roc_auc_list)\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective_func,\n",
    "            space=xgb_search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng()\n",
    "           )\n",
    "print('best:', best)\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'], 5),\n",
    "                        max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']), \n",
    "                        colsample_bytree=round(best['colsample_bytree'], 5), random_state=RANDOM_STATE)\n",
    "\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric=\"logloss\",eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0:.4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd304a8-146a-4163-9a94-e8a133a0cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_clf.predict(X_train) \n",
    "proba = xgb_clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = xgb_clf.predict(X_test) \n",
    "best_rf_proba = xgb_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test , best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fcf037-9f17-4535-8e31-9e70b8ac9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_xgb_hyper = xgb.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_xgb_hyper\n",
    "gender_submission_df.to_csv('titanic_submission_xgb_hyper.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad35d8f-a56c-4904-b7c2-ca00df267a21",
   "metadata": {},
   "source": [
    "# lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c7585-e889-437c-8dc1-9cfd567effff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import early_stopping\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=RANDOM_STATE)\n",
    "# lgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "lgbm.fit(X_tr, y_tr, callbacks=[early_stopping(stopping_rounds=50)], eval_metric=\"logloss\", eval_set=evals)\n",
    "\n",
    "preds = lgbm.predict(X_test)\n",
    "pred_proba = lgbm.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e8e60-eadb-4a46-85f8-0d7874862219",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lgbm.predict(X_train) \n",
    "proba = lgbm.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = lgbm.predict(X_test) \n",
    "best_rf_proba = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test , best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b71bb-9afb-4b19-8728-0a40de75aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_lgbm = lgbm.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_lgbm\n",
    "gender_submission_df.to_csv('titanic_submission_lgbm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3fc88-d443-421a-aca4-914be6473ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 60),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)\n",
    "    }\n",
    "\n",
    "    lgb_model = LGBMClassifier(**param, random_state=RANDOM_STATE, verbose=-1)\n",
    "    lgb_model.fit(X_train, y_train, feature_name=['f' + str(i) for i in range(X_train.shape[1])])\n",
    "    y_val_pred = lgb_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_val_pred, pos_label=1) \n",
    "    return f1\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "best_lgb_model = LGBMClassifier(**best_params, random_state=RANDOM_STATE)\n",
    "best_lgb_model.fit(X_train, y_train, feature_name=['f' + str(i) for i in range(X_train.shape[1])])\n",
    "\n",
    "y_val_pred = best_lgb_model.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, y_val_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_val_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_val_pred, pos_label=1)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_val_pred, pos_label=1)}\")  \n",
    "print(f\"Recall: {recall_score(y_test, y_val_pred, pos_label=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c09784-2df2-481d-87fa-1b45b00ef446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_lgb_model.predict(X_train) \n",
    "proba = best_lgb_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = best_lgb_model.predict(X_test) \n",
    "best_rf_proba = best_lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test , best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c66a0b-6dbb-46ea-998e-a1595e201a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_lgbm_clf = best_lgb_model.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_lgbm_clf\n",
    "gender_submission_df.to_csv('titanic_submission_lgbm_clf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724016c-f784-4a24-9a24-3f46f6965df0",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b86b6-7a36-4006-b4e4-c8590fcee387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat = CatBoostClassifier(random_state=RANDOM_STATE)\n",
    "cat.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22b81d-a1ca-4acf-9572-409c6500efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cat.predict(X_train) \n",
    "proba = cat.predict_proba(X_train)[:, 1]\n",
    "\n",
    "best_rf_pred = cat.predict(X_test) \n",
    "best_rf_proba = cat.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, pred, proba)\n",
    "get_clf_eval(y_test , best_rf_pred, best_rf_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b554e-77c8-4baa-b191-dbb4355dd254",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_cat = cat.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_cat\n",
    "gender_submission_df.to_csv('titanic_submission_cat.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1dc1a-678b-4f5c-80fa-2dc1b206c8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    iterations = trial.suggest_int('iterations', 100, 1000, step=10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.1, 1.0, step=0.1)\n",
    "    depth = trial.suggest_int('depth', 3, 15)\n",
    "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 0.1, 10.0)\n",
    "    bagging_temperature = trial.suggest_float('bagging_temperature', 0.0, 1.0)\n",
    "    class_weight = trial.suggest_float('class_weight', 1.0, 50.0)\n",
    "    random_strength = trial.suggest_float('random_strength', 0.0, 10.0)\n",
    "    od_wait = trial.suggest_int('od_wait', 10, 50)\n",
    "\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=iterations,\n",
    "        learning_rate=learning_rate,\n",
    "        depth=depth,\n",
    "        l2_leaf_reg=l2_leaf_reg,\n",
    "        bagging_temperature=bagging_temperature,\n",
    "        class_weights=[1, class_weight],\n",
    "        random_strength=random_strength,\n",
    "        od_wait=od_wait,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_val_pred, pos_label=0)\n",
    "    print(f\"Trial {trial.number} finished with F1 score: {f1}\")\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best params: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ee0ea-600e-4faa-8a14-be53a775a6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'class_weight' in best_params:\n",
    "    best_params['class_weights'] = [1, best_params.pop('class_weight')]\n",
    "\n",
    "best_cat_model = CatBoostClassifier(**best_params, random_state=RANDOM_STATE)\n",
    "best_cat_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac74821-8023-47c3-94d5-99ffd1dc0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_cat_model.predict(X_train)\n",
    "y_train_pred_proba = best_cat_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "y_test_pred = best_cat_model.predict(X_test)\n",
    "y_test_pred_proba = best_cat_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_train, y_train_pred, y_train_pred_proba)\n",
    "get_clf_eval(y_test, y_test_pred, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefd5a0-da7c-42a0-bbfe-fd98d090bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_lgbm = best_cat_model.predict(predict_df)\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_lgbm\n",
    "gender_submission_df.to_csv('titanic_submission_cat_hyper.csv',index=False)\n",
    "gender_submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5d045-a19e-4597-ac65-1c36a2aaa23f",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b23bd-e4d4-49a4-8146-f522e1a7427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.round(y_pred)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    tp = K.sum(y_true * y_pred)\n",
    "    fp = K.sum((1 - y_true) * y_pred)\n",
    "    fn = K.sum(y_true * (1 - y_pred))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
    "    return f1\n",
    "\n",
    "\n",
    "def find_best_threshold(y_test, pred_proba):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "    f1_scores = [f1_score(y_test, (pred_proba >= t).astype(int)) for t in thresholds]\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    best_f1_score = np.max(f1_scores)\n",
    "    return best_threshold, best_f1_score\n",
    "\n",
    "\n",
    "# 드롭아웃(Dropout)을 여러 층에 추가하는 것은 과적합을 방지하기 위한 좋은 방법\n",
    "# BatchNormalization은 배치 정규화로, 각 층의 입력을 정규화하여 학습을 안정화하고 빠르게 만드는 역할 / 과적합을 방지\n",
    "def model_fn(a_layer=None):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(7, )))\n",
    "\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    \n",
    "    if a_layer: #케라스 층을 추가하면 은닉층 뒤에 은닉층 추가\n",
    "        model.add(a_layer)\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ca404-133d-417f-a457-9d59d5af595a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = model_fn(keras.layers.Dropout(0.3))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_score])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.keras', save_best_only=True)\n",
    "# early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True), , early_stopping_cb\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, verbose=1, validation_data=(X_test, y_test), callbacks=[checkpoint_cb])\n",
    "\n",
    "print(history.history.keys())\n",
    "print(early_stopping_cb.stopped_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c800543-fbb2-4954-b0ce-2b3bb28e8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train, y_train)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce40698f-94d3-48de-bcfb-99d031e9ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 numpy 배열로 변환 및 데이터 타입을 float32로 변경\n",
    "x_train = X_train.to_numpy().astype('float32')\n",
    "x_test = X_test.to_numpy().astype('float32')\n",
    "\n",
    "y_train = y_train.to_numpy().astype('float32')\n",
    "\n",
    "# 변환된 데이터의 shape를 출력\n",
    "print('x_train.shape = ', x_train.shape, ', x_test.shape = ', x_test.shape, ', y_train.shape = ', y_train.shape)\n",
    "\n",
    "# 출력 결과\n",
    "# x_train.shape = (891, 12), x_test.shape = (418, 12), y_train.shape = (891, 1)\n",
    "\n",
    "# TensorFlow 모델을 정의\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "kaggle_titanic_ann_model = Sequential()\n",
    "\n",
    "# 모델에 레이어 추가\n",
    "kaggle_titanic_ann_model.add(Dense(64, activation='sigmoid', input_shape=(x_train.shape[1],)))\n",
    "kaggle_titanic_ann_model.add(Dropout(0.25))\n",
    "\n",
    "kaggle_titanic_ann_model.add(Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fbf25-2016-4d28-bb08-26a4fff2679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "kaggle_titanic_ann_model.compile(optimizer=SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "hist = kaggle_titanic_ann_model.fit(x_train, y_train, epochs=200)\n",
    "\n",
    "# 테스트 데이터에 대한 생존 예측\n",
    "x_test = predict_df.to_numpy().astype('float32')\n",
    "survived_prediction = kaggle_titanic_ann_model.predict(x_test)\n",
    "print(survived_prediction.shape)\n",
    "\n",
    "# 예측된 값을 Argmax로 변환하여 각 행에 대해 가장 높은 확률을 가진 클래스를 선택\n",
    "survived_prediction_digit = np.argmax(survived_prediction, axis=1)\n",
    "\n",
    "# 예측 결과를 DataFrame에 추가하고 CSV 파일로 저장\n",
    "gender_submission_df['Survived'] = survived_prediction_digit\n",
    "gender_submission_df.to_csv('Kaggle_Titanic_Competition.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8640fa-4f13-4d76-b11a-15007b0e6fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fe44b-c1a3-4e69-aea7-86135cf829e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_titanic_pred_ann = model.predict(predict_df)\n",
    "\n",
    "predict_titanic_pred_ann = (predict_titanic_pred_ann > 0.5).astype(int)\n",
    "predict_titanic_pred_ann = pd.DataFrame(predict_titanic_pred_ann, columns=['Survived'])\n",
    "\n",
    "\n",
    "gender_submission_df['Survived'] = predict_titanic_pred_ann\n",
    "gender_submission_df.to_csv('titanic_submission_ann.csv',index=False)\n",
    "gender_submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498fe2d-b57f-4f94-b6b9-22bf8fbf207d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
